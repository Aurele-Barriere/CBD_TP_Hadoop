As supercomputers become larger and more complex, one critical challenge is to efficiently handle the immense amounts of data generated by extreme-scale simulations. The traditional approach to data management consists of writing data to a parallel file system, using a high-level I/O library on top of a standardized interface such as MPI-I/O. This data is then read back for analysis and visualization purpose.

One major issue posed by this traditional approach to data management is that it induces a high performance variability. This variability can be observed at different levels. Within a single application, I/O contention across processes leads to large variations in the time each process takes to complete its I/O operations (I/O jitter). Such differences from a process to another in a massively parallel application makes all processes wait for the slowest one. These processes thus waste valuable computation time. The variability is even larger from one I/O phase to another, due to interference with other applications sharing the same parallel file system. 

While scientists have found a potential solution to this problem by coupling their simulations with visualization software in order to bypass data storage and derive results early on, the current practices of coupling simulations with visualization tools also expose simulations to high performance variability, as their run time does not depend anymore on their own scalability only, but also on the scalability of visualization algorithms. This particular problem is further amplified in the context of interactive in situ visualization, where the user himself and his interactions with the simulation become the cause of run-time variability. 

To make an efficient use of future exascale machines, it becomes important to provide data management solutions that do not solely focus on pure performance, but address performance variability as well. Addressing this variability is indeed the key to ensure that each and every component of these future platforms is optimally used.

To address these challenges, we have proposed a new system for I/O and data management called Damaris. Damaris leverages dedicated I/O cores on each multicore SMP (Symmetric multiprocessing) node, along with the use of shared memory, to efficiently perform asynchronous data processing I/O and in situ visualization. We picked this approach based on the intuition that the usage of dedicated cores for I/O-related tasks combined with the usage of intranode shared memory can help overlapping I/O with computation, but also lowering the pressure on the storage system by reducing the number of files to be stored and, at the same time, the amount of data. Such dedicated resources can indeed perform data aggregation, filtering or compression, all in an asynchronous manner. Moreover, such dedicated cores can further be leveraged to enable non-intrusive in situ data visualization with optimized resource usage. Some of these aspects of the Damaris approach have been introduced in previous conference papers~\cite{dorier2012damaris,dorier2013damarisviz}. This paper aims to provide a comprehensive, global presentation and discussion of the Damaris approach in its current state and of its evaluation and applications.

We evaluated Damaris on three different platforms including the Kraken Cray XT5 supercomputer~\cite{kraken}, with the CM1 atmospheric model~\cite{bryan2002benchmark} and the Nek5000~\cite{nek5000} computational fluid dynamics code. By overlapping I/O with computation and by gathering data into large files while avoiding synchronization between cores, our solution brings several benefits: (1) it fully hides the jitter as well as all I/O-related costs, which makes the simulationâ€™s performance predictable; (2) it substantially increases the sustained write throughput (by a factor of 15 in CM1, 4.6 in Nek5000) compared with standard approaches; (3) it allows almost perfect scalability of the  simulation (up to over 9,000 cores with CM1 on Kraken), as opposed to state-of-the-art approaches which fail to scale; (4) it enables data compression without any additional overhead, leading to a major reduction of storage requirements.

Furthermore, we extended Damaris with Damaris/Viz, an in situ visualization framework based on the Damaris approach. By leveraging dedicated cores, external high-level structure descriptions and a simple API, our framework provides adaptable in situ visualization to existing simulations at a low instrumentation cost. Results obtained with the Nek5000 and CM1 simulations show that our framework can completely hide the performance impact of visualization tasks and the resulting run-time variability. In addition, the proposed API allows efficient memory usage through a shared-memory-based, zero-copy communication model.

Finally, in order to compare the Damaris, dedicated-core-based approach with other approaches such as dedicated nodes, forwarding nodes, and staging areas, we further extended Damaris to support the use of dedicated nodes as well. We leverage again the CM1 and Nek5000 simulations on Grid'5000, the national French grid testbed, to shed light on the conditions under which a dedicated-core-based approach to I/O is more suitable than a dedicated-node-based one, and vice versa.

To the best of our knowledge, Damaris is the first open-source middleware to enable the use of dedicated cores or/and dedicated nodes for data management tasks ranging from storage I/O to complex in situ visualization scenarios.

The rest of this paper is organized as follows: Section~\ref{sec:background} presents the background and motivation for our work, discusses the limitations of current approaches to I/O and to in situ visualization. Our Damaris approach, including its design principles, implementation detail and use cases, is described in Section~\ref{sec:damaris}. We evaluate Damaris in Section~\ref{sec:evaluation}, first in scenarios related to storage I/O, then in scenarios related to in situ visualization. Our experimental evaluation continues in Section~\ref{sec:discussion} with a comparison between dedicated cores and dedicated nodes in various situations. Section~\ref{sec:related} discusses our positioning with respect to related work and Section~\ref{sec:conclusion} summarizes our conclusions and discusses open further directions.


